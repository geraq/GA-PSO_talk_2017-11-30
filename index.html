<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Word Embeddings for natural language processing</title>

		<meta name="description" content="Word Embeddings for natural language processing">
		<meta name="author" content="Germán Aquino">

		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/white.css" id="theme">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
		<!--
		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
		</script>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/0.1/MathJax.js?config=TeX-MML-AM_CHTML'></script>
		-->
		<style>
			h3 {text-transform: none !important;}
		</style>
	</head>

	<body>

		<div class="reveal">
			<header style="position: absolute;top: 50px; left: 100px"></header>
			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
					<h3>Word embeddings for natural language processing</h3>
				</section>
				<section style="font-size: 25px">
					<!--<style>header:after { content: "Outline"; }</style>-->
						<h3> Outline </h3>
						<ul>
							<li>Introduction</li>
							<li>Applications of word embeddings</li>
							<li>Language models
								<ul>
									<li>count-based</li>
									<li>prediction-based (NN-based)</li>
								</ul>
							</li>
							<li>Neural network language models</li>
							<li>Word2vec
								<ul>
									<li>Context window</li>
									<li>Architectures
										<ul>
											<li>Cbow</li>
											<li>Skipgram</li>
										</ul>
									</li>
									<li>Optimizations
										<ul>
											<li>Hierarchical Softmax</li>
											<li>Negative Sampling</li>
										</ul>
									</li>
									<!--
									<li>Other optimizations
										<ul>
											<li>random window length, multiword terms, infrequent terms removal, frequent terms undersampling.</li>
										</ul>
									</li>
									-->
								</ul>
							</li>
							<li>Implementation</li>
							<li>Limitations</li>
							<li>Extensions
								<ul>
									<li>Doc2vec</li>
									<li>FastText</li>
								</ul>
							</li>
						</ul>
				</section>
				<section>
					<h3>Word Embeddings</h3>
						<ul>
							<li>Analogy tasks: Paris is to France as Berlin is to <i>what</i>?</li>
							<li>Find the word that does not fit: <br>
								<i>dinner cereal breakfast lunch</i>
							</li>
						</ul>
				</section>
				<section>
					<h3>Word Embeddings</h3>
					<img height="100%" width="100%" data-src="./images/word_embeddings.png" alt="Word Embeddings" style="border:none; box-shadow:none;" />
				</section>
				<section>
					<h3>Applications</h3>
					<img height="50%" width="50%" data-src="./images/vector_algebra.png" alt="Semantic properties as vector algebra" style="border:none; box-shadow:none;" />
					<p style="font-size:50%">Distributed Representations of Words and Phrases and their Compositionality (Mikolov 2013)</p>
				</section>
				<section>
					<h3>Applications</h3>
					<img height="100%" width="100%" data-src="./images/mikolov_analogies.png" alt="Analogy tasks (Mikolov)" style="border:none; box-shadow:none;" />
					<p style="font-size:50%">Efficient Estimation of Word Representations in Vector Space (Mikolov 2013)</p>
				</section>
				<section>
					<h3>Applications</h3>
					<img height="100%" width="100%" data-src="./images/collobert_table.png" alt="Most related words (Collobert)" style="border:none; box-shadow:none;" />
					<p style="font-size:50%">Natural Language Processing (almost) from Scratch (Collobert and Weston 2011)</p>
				</section>
				<section>
					<h3>Applications</h3>
					<img height="70%" width="70%" data-src="./images/mikolov_machine_translation.png" alt="Embeddings for machine translation (Mikolov)" style="border:none; box-shadow:none;" />
					<p style="font-size:50%">Exploiting Similarities among Languages for Machine Translation (Mikolov 2013)</p>
				</section>
				<section>
					<h3>Applications</h3>
					<img height="70%" width="70%" data-src="./images/socher_image_embeddings.png" alt="Embeddings for image classification (Socher)" style="border:none; box-shadow:none;" />
					<p style="font-size:50%">Zero-Shot Learning Through Cross-Modal Transfer (Socher 2013)</p>
				</section>
				<!--
				<section data-background-iframe="https://rare-technologies.com/word2vec-tutorial/#bonus_app">
				-->
				<section>
					<h3>Bag-of-words, TF-IDF and LSA</h3>
					<!--<img data-src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f6ce07d37d7cc1057be2b032cccc5cf580945af9"-->
					<img data-src="./images/lsa.svg" alt="Latent Semantic Analysis" style="border:none; box-shadow:none;" />
					<div style="font-size:60%">
						<p>$x_{i,j} = TFIDF(t_i, d_j)$</p>
						<p>$TFIDF(t_i, d_j) = TF(t_i, d_j) ⋅ IDF(t_i, d_j)$</p>
						<p>$TF(t_i, d_j) = \frac{freq(t_i, d_j)}{|d_j|}$</p>
						<p>$IDF(t_i, d_j) = log(\frac{|D|}{|\{d_k : t_i ∈ d_k\}|})$</p>
				</div>
				</section>
				<section>
					<h3>Bag-of-words and TF-IDF</h3>
					<div style="text-align:left; padding: 0% 10% 0% 20%;font-size: 80%">
						<p>Doc1 = "He is a lazy boy. She is also lazy." </p>
						<p>Doc2 = "John is a lazy person." </p>
					</div>
					<div>
						<div style="font-size:50%;display:inline-block;margin-right:10%">
							<p>Raw frequencies</p>
							<table>
								<tr><td></td><td>Doc1</td><td>Doc2</td></tr>
								<tr><td>a</td><td>1</td><td>1</td></tr>
								<tr><td>also</td><td>1</td><td>0</td></tr>
								<tr><td>boy</td><td>1</td><td>0</td></tr>
								<tr><td>he</td><td>1</td><td>0</td></tr>
								<tr><td>is</td><td>2</td><td>1</td></tr>
								<tr><td>john</td><td>0</td><td>1</td></tr>
								<tr><td>lazy</td><td>2</td><td>1</td></tr>
								<tr><td>person</td><td>0</td><td>1</td></tr>
								<tr><td>she</td><td>1</td><td>0</td></tr>
							</table>
						</div>
						<div style="font-size:50%;display:inline-block">
							<p>TF-IDF</p>
							<table >
								<tr><td></td><td>Doc1</td><td>Doc2</td></tr>
								<tr><td>a</td><td>0</td><td>0</td></tr>
								<tr><td>also</td><td>0.077</td><td>0</td></tr>
								<tr><td>boy</td><td>0.077</td><td>0</td></tr>
								<tr><td>he</td><td>0.077</td><td>0</td></tr>
								<tr><td>is</td><td>0</td><td>0</td></tr>
								<tr><td>john</td><td>0</td><td>0.138</td></tr>
								<tr><td>lazy</td><td>0</td><td>0</td></tr>
								<tr><td>person</td><td>0</td><td>0.138</td></tr>
								<tr><td>she</td><td>0.077</td><td>0</td></tr>
							</table>
						</div>
					</div>
				</section>
				<section>
					<h3>Latent Semantic Analysis: decomposition through SVD</h3>
					<img data-src="./images/svd.png" alt="Singular Value Decomposition" style="border:none; box-shadow:none;" />
				</section>
				</section>
				<section>
					<h3>Language models</h3>
					<p>Given vocabulary $V$,</p>
					<p>approximate a conditional distribution over words:</p>
					<p>$$ P(W_o | W_i), W_i \in V, W_o \in V$$</p>
					<p>$$ P(W_{i+1} | {W_{i-n}, ..., W_{i-1}, W_i}), W_j \in V, j \in \mathbb{N} $$</p>
				</section>
				<section>
					<h3>Distributional hypothesis</h3>
					<div>
						<p>The <span style="color:red">cat</span> is <span style="color:blue">walking</span> in the <span style="color:green">bedroom</span></p>
						<p>A <span style="color:red">dog</span> was <span style="color:blue">running</span> in a <span style="color:green">room</span></p>
					</div>
				</section>
				<section>
					<h3>Count-based language models</h3>
					<ul>
						<li>N-grams</li>
						<li>co-occurrence count on training set</li>
						<li>Probability smoothing</li>
						<li>The amount of data required grows exponentially with the size of the context</li>
					</ul>
				</section>
				<section>
					<h3>Neural network language models</h3>
					<ul>
						<li>Distributed representations</li>
						<li>Embeddings are represented by the activation pattern of the hidden layer as a whole</li>
						<li>Words with similar meanings are represented by vectors that are close in the high-dimensional space</li>
					</ul>
				</section>
				<section>
					<h3>Neural network language models</h3>
					<ul>
						<li>Bengio 2003</li>
						<li>Morin, Bengio 2005</li>
						<li>Collobert, Weston 2008 (CNNs)</li>
						<li>Mnih, Hinton 2009 (HLBL)</li>
						<li>Mikolov 2012 (RNNs)</li>
						<li><b><i><span style="color:red">Mikolov 2013 (word2vec)<span></i></b></li>
						<li>Mnih, Kavukcuoglu (vLBL, ivLBL) 2013</li>
						<li>Pennington, Socher 2014 (GloVe)</li>
					</ul>
				</section>
				<section>
					<h3>Word2vec</h3>
					<img data-src="./images/word2vec_layout.png" alt="Word2vec neural network" style="border:none; box-shadow:none;" />
				</section>
				<section>
					<h3>Word2vec</h3>
					<img height="100%" width="100%" data-src="./images/mat_mult2.png" alt="One-hot matrix multiplication" style="border:none; box-shadow:none;" />
				</section>
				<section data-background-iframe="https://ronxin.github.io/wevi/"></section>
				<section>
					<h3>Sliding window</h3>
					<img height="60%" width="60%" data-src="./images/sliding_window.png" alt="Word2vec sliding window" style="border:none; box-shadow:none;" />
				</section>
				<section>
					<h3>Architectures</h3>
					<table>
						<tr>
							<td>
								<div style="text-align:center">
									<img id="cbow" data-src="./images/cbow.png" alt="Word2vec CBOW" style="border:none; box-shadow:none;" />
									<label for="cbow">CBOW</label>
								</div>
							</td>
							<td>
								<div style="text-align:center">
									<img id="skipgram" data-src="./images/skipgram.png" alt="Word2vec Skipgram" style="border:none; box-shadow:none;" />
									<label for="skipgram">Skipgram</label>
								</div>
							</td>
						</tr>
					</table>
				</section>
				<section>
					<h3>Word2vec</h3>
					<p>Output probabilities are modeled using the <i>softmax</i> function</p>
					<p>$x = [x_1, x_2, ..., x_n]$</p>
					<p>$$softmax(x_i) = \frac{e^{x_i}}{\sum_{j}{e^{x_j}}}$$</p>
					<p>Cross-entropy is used as loss</p>
					<p>$E(y,t) = - \sum_{m=1}^{V}{t_m \cdot log(y_m)} = - log(y_o)$</p>
				</section>
				<section>
					<h3>Word2vec</h3>
					<p>Given an input word $W_k$ and an output word $W_j$ in the context of $W_k$,</p>
					<p>let $x$ be the one-hot representation for $W_k$</p>
					<p>$x = [x_1, x_2, ..., x_k, ...,x_V], x_k = 1, x_n = 0, n \neq k$</p>
					<p>$t = [t_1, t_2, ..., t_j, ...,t_V]$</p>
					<p>$t_j = 1$</p>
					<p>$t_n = 0, n \neq j$</p>
				</section>
				<section>
					<h3>Word2vec</h3>
					<p>activation $h = x^TW = W_{(k, :)} := v_{w_i}$</p>
					<p>score $u_j = {v'}_{w_j}^{T} ⋅ h$, where ${v'}_{w_j}^{T} = {W'}_{(:,j)}$</p>
					<p>$P(w_j | w_i) = y_j = \frac{exp(u_j)}{\sum_{j'=1}^{V}{exp(u_{j'})}}$</p>
				</section>
				<section>
					<h3>Word2vec</h3>
					<div style="font-size:80%">
						<p>$E(y,t) = - \sum_{m=1}^{V}{t_m \cdot log(y_m)} = - log(y_o)$</p>
						<p>$\frac{∂E}{∂{w'}_{ij}} = \frac{∂E}{∂u_j} ⋅ \frac{∂u_j}{∂{w'}_{ij}} = (y_j - t_j) ⋅ h_i$</p>
						<p style="color:blue">${v'}_{w_{ij}}^{new} ← {v'}_{w_{ij}}^{old} - η ⋅ (y_j - t_j) ⋅ h_i$</p>
						<p>$\frac{∂E}{∂w_{ki}} = \sum_{j=1}^{V}{(\frac{∂E}{∂u_j} ⋅ \frac{∂u_j}{∂h_i})} ⋅ \frac{∂h_i}{∂w_{ki}} =  \\
							= \sum_{j=1}^{V}{((y_j - t_j) ⋅ w'_{ij})} ⋅ x_k$</p>
						<p style="color:blue">$v^{(new)}_{ki} ← v^{(old)}_{ki} - η ⋅ \sum_{j=1}^{V}{((y_j - t_j) ⋅ w'_{ij})} ⋅ x_k$</p>
					</div>
				</section>
				<section>
					<h3>Neural network language models</h3>
					<img height="50%" width="50%" data-src="./images/bengio2003.png" alt="Bengio 2003 model" style="border:none; box-shadow:none;" />
					<p style="font-size:30px">A Neural Probabilistic Language Model (Bengio 2003)</p>
				</section>
				<section>
					<h3>Optimizations</h3>
					<ul>
						<li>Hierarchical Softmax</li>
						<li>Negative Sampling</li>
					</ul>
				</section>
				<section>
					<h3>Hierarchical Softmax</h3>
					<img height="60%" width="60%" data-src="./images/hierarchical_softmax.png" alt="Word2vec Hierarchical Softmax" style="border:none; box-shadow:none;" />
					<div style="text-align:center; font-size:60%">
						<ul>
							<li> $h$ = hidden layer activation</li>
							<li> $n(w,j)$ = jth node in the path of $w$</li>
							<li> $ch(n(w, j))$ = left child of $n(w,j)$</li>
							<li> $v'_{n(w,j)}$ = output neuron corresponding to $n(w,j)$</li>
						</ul>
					</div>
				</section>
				<section>
					<h3>Hierarchical Softmax</h3>
					<img height="75%" width="75%" data-src="./images/hierarchical_softmax2.png" alt="Word2vec Hierarchical Softmax" style="border:none; box-shadow:none;" />
				</section>
				<section>
					<h3>Hierarchical Softmax, Huffman tree</h3>
					<img height="40%" width="40%" data-src="./images/HuffmanCodeAlg.png" alt="Huffman Tree" style="border:none; box-shadow:none;" />
				</section>
				<section>
					<h3>Hierarchical Softmax</h3>
					<img height="70%" width="70%" data-src="./images/hs_update_rule.png" alt="Word2vec Hierarchical Softmax update rule" style="border:none; box-shadow:none;" />
				</section>
				<section>
					<h3>Negative Sampling</h3>
					<ul>
						<li>Pick up <span style="color:green">K</span> incorrect words</li>
						<li><span style="color:blue">Maximize</span> the probability of picking the <span style="color:blue">right</span> word</li>
						<li><span style="color:red">Minimize</span> the probability of picking <span style="color:red">incorrect</span> words</li>
						<li>Pick negative samples with probability proportional to their frequency, raised to the 3/4 power</li>
						<li>Unlike Hierarchical softmax, the output layer does not model a probability distribution</li>
					</ul>
				</section>
				<section>
					<h3>Negative Sampling</h3>
					<img height="90%" width="90%" data-src="./images/ns_update_rule.png" alt="Word2vec Negative Sampling update rule" style="border:none; box-shadow:none;" />
				</section>
				<section>
					<h3>Implementation</h3>
					<p style="font-size:50%">$ ./word2vec
						-<span style="color:blue">train</span> data.txt
						-<span style="color:blue">output</span> vectors.txt
						-<span style="color:blue">size</span> 200
						-<span style="color:blue">window</span> 5
						-<span style="color:blue">cbow</span> 1
						-<span style="color:blue">hs</span> 0
						-<span style="color:blue">negative</span> 5 <br>
						-<span style="color:blue">sample</span> 1e-4
						-<span style="color:blue">alpha</span> 0.0025
						-<span style="color:blue">min-count</span> 10
						<!-- -<span style="color:blue">binary</span> 0
						 -<span style="color:blue">debug</span> 2-->
					</p>
					<ul style="font-size:50%">
						<li>min-count: minimum frequency a words needs to have, otherwise it is discarded (default=5)</li>
						<li>sample: coefficient to discard frequent words, with probability proportional to their frequency (default = 0, 1e-5 is a good value)
								<p>In the paper: $P(w_i) = 1 - \sqrt{\frac{sample}{f(w_i)}}$</p>
								<p>In the code: $P(w_i) = 1 - \big({\sqrt{\frac{sample \cdot |V|}{f(w_i)}} + \frac{sample \cdot |V|}{f(w_i)}}\big)$</p>
						</li>
					</ul>
				</section>
				<section>
					<h3>Multi-words terms</h3>
					<p>$ ./word2phrase -<span style="color:blue">train</span> text.txt
						-<span style="color:blue">output</span> phrases.txt
						-<span style="color:blue">threshold</span> 100
						-<span style="color:blue">min-count</span> 10
					<p>Join two adjacent words with an underscore if their joint score is higher than <i>threshold</i>:</p>
					<p>$score(w_i, w_j) = \frac{(count(w_i, w_j) - min\_count) \times |V|}{count(w_i) \times count(w_j)}$</p>
				</section>
				<section>
					<h3>Limitations</h3>
						<ul>
							<li>Out-of-vocabulary words</li>
							<li>Ambiguity</li>
							<li>Temporal ordering</li>
						</ul>
				</section>
				<section>
					<h3>Extensions</h3>
				</section>
				<section>
					<h3>Doc2Vec</h3>
						<img height="80%" width="80%" data-src="./images/doc2vec.png" alt="Doc2vec" style="border:none; box-shadow:none;" />
						<p style="font-size:50%">Distributed Representations of Sentences and Documents (Le and Mikolov 2014)</p>
				</section>
				<section>
					<h3>FastText</h3>
					<ul>
						<li>Word vectors are composed by character n-grams</li>
						<li>Vectors for out-of-vocabulary words can be approximated from the character n-gram vectors</li>
						<li>Better for syntactic tasks and highly-inflected languages</li>
					</ul>
				</section>
				<section>
					<h3>FastText, out-of-vocabulary words</h3>
						<p><i>The cat is walking in the bedroom</i></p>
						<p style="font-size:70%">$\{the, cat, is, walking, wal, walk, walki, alk, alki, alkin, ..., \\
							in, the, bedroom, bed, bedr, bedro, edro, edroo, ...\}$</p>
				</section>
				<section>
					<h3>FastText, out-of-vocabulary words</h3>
					<p style="font-size:60%">$ ./fasttext
						<span style="color:blue">skipgram</span>
						-<span style="color:blue">input</span> text8
						-<span style="color:blue">output</span> text8_model
						-<span style="color:blue">epoch</span> 1
						-<span style="color:blue">lr</span> 0.5 <br>
						-<span style="color:blue">minn</span> 2
						-<span style="color:blue">maxn</span> 5
						-<span style="color:blue">dim</span> 300
					</p>
					<p style="font-size:60%">$ ./fasttext <span style="color:blue">nn</span> text8_model.bin</p>
					<p style="font-size:60%;white-space: pre-line">
						<i>Query word? gearshift</i>
						gearing 0.790762
						flywheels 0.779804
						flywheel 0.777859
						gears 0.776133
						driveshafts 0.756345
						driveshaft 0.755679
						daisywheel 0.749998
						wheelsets 0.748578
						epicycles 0.744268
						gearboxes 0.73986
				</p>
				</section>
				<section>
					<h3>FastText, supervised training for text classification</h3>
					<p style="font-size:50%; white-space: pre-line;text-align:left;">
						<span style="color:blue;font-style:italic">__label__sauce __label__cheese</span> how much does potato starch affect a cheese sauce recipe
						<span style="color:blue;font-style:italic">__label__food-safety __label__acidity</span> dangerous pathogens capable of growing in acidic environments
						<span style="color:blue;font-style:italic">__label__cast-iron __label__stove</span> how do i cover up the white spots on my cast iron stove
						<span style="color:blue;font-style:italic">__label__restaurant</span> michelin three star restaurant; but if the chef is not there
						<span style="color:blue;font-style:italic">__label__knife-skills __label__dicing</span> without knife skills how can i quickly and accurately dice vegetables
						<span style="color:blue;font-style:italic">__label__storage-method __label__equipment __label__bread</span> whats the purpose of a bread box
						<span style="color:blue;font-style:italic">__label__baking __label__food-safety __label__substitutions __label__peanuts</span> how to seperate peanut oil from roasted peanuts at home
						<span style="color:blue;font-style:italic">__label__chocolate</span> american equivalent for british chocolate terms
						<span style="color:blue;font-style:italic">__label__baking __label__oven __label__convection</span> fan bake vs bake
						<span style="color:blue;font-style:italic">__label__sauce __label__storage-lifetime __label__acidity __label__mayonnaise</span> regulation and balancing of readymade packed mayonnaise and other sauces
					</p>
				</section>
				<section>
					<h3>FastText, supervised training for text classification</h3>
					<p style="font-size:70%">$ ./fasttext
						<span style="color:blue">supervised</span>
						-<span style="color:blue">input</span> cooking.train
						-<span style="color:blue">output</span> model_cooking
						-<span style="color:blue">lr</span> 1.0 <br>
						-<span style="color:blue">epoch</span> 5
						-<span style="color:blue">wordNgrams</span> 2
					</p>
					<p style="font-size:70%">$ ./fasttext
						<span style="color:blue">predict-prob</span> model_cooking.bin - 5</p>
					<p style="font-size:70%"><i>Query: Why not put knives in the dishwasher?</i></p>
					<p style="white-space: pre-line;font-size:70%">__label__knives 0.335938
						__label__equipment 0.0664063
						__label__food-safety 0.0664063
						__label__cleaning 0.0605469
						__label__storage-method 0.0332031
					</p>
				</section>
				<section>
					<h3>Questions?</h3>
				</section>

			</div> <!-- slides -->

		</div> <!-- reveal -->

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// More info https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,
				slideNumber: true,
				overview: true,

				transition: 'none', // none/fade/slide/convex/concave/zoom

				math: {
					mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
					config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
				},

				// More info https://github.com/hakimel/reveal.js#dependencies
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/math/math.js', async: true }
				]

			});

		</script>

	</body>
</html>
